{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "9주차_PySpark_기본_2일차.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/learn-programmers/programmers_kdt_II/blob/main/9%EC%A3%BC%EC%B0%A8_PySpark_%EA%B8%B0%EB%B3%B8_2%EC%9D%BC%EC%B0%A8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAqhTDfuWrcM"
      },
      "source": [
        "PySpark을 로컬머신에 설치하고 노트북을 사용하기 보다는 머신러닝 관련 다양한 라이브러리가 이미 설치되었고 좋은 하드웨어를 제공해주는 Google Colab을 통해 실습을 진행한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XIA23YgbXKJd"
      },
      "source": [
        "이를 위해 pyspark과 Py4J 패키지를 설치한다. Py4J 패키지는 파이썬 프로그램이 자바가상머신상의 오브젝트들을 접근할 수 있게 해준다. Local Standalone Spark을 사용한다.(서버가 하나있는 개발용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NbT0rpGfVdiq",
        "outputId": "b2355d15-348d-4513-e185-be045435366f"
      },
      "source": [
        "!pip install pyspark==3.0.1 py4j==0.10.9 "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyspark==3.0.1\n",
            "  Downloading pyspark-3.0.1.tar.gz (204.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 204.2 MB 31 kB/s \n",
            "\u001b[?25hCollecting py4j==0.10.9\n",
            "  Downloading py4j-0.10.9-py2.py3-none-any.whl (198 kB)\n",
            "\u001b[K     |████████████████████████████████| 198 kB 17.2 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyspark\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612242 sha256=01a53577218b654b7baed808dbc55099b5a0dee39c023c8887b63d760f1384e8\n",
            "  Stored in directory: /root/.cache/pip/wheels/5e/34/fa/b37b5cef503fc5148b478b2495043ba61b079120b7ff379f9b\n",
            "Successfully built pyspark\n",
            "Installing collected packages: py4j, pyspark\n",
            "Successfully installed py4j-0.10.9 pyspark-3.0.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQIB8vLPBMeu",
        "outputId": "30070266-ba7f-4cc8-d761-332eb9885eaa"
      },
      "source": [
        "!ls -tl"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4\n",
            "drwxr-xr-x 1 root root 4096 Jul 16 13:20 sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHXY3xYctlCe",
        "outputId": "29460939-2ef3-46f5-b95c-e052893addcb"
      },
      "source": [
        "!ls -tl sample_data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 55508\n",
            "-rw-r--r-- 1 root root 18289443 Jul 16 13:20 mnist_test.csv\n",
            "-rw-r--r-- 1 root root 36523880 Jul 16 13:20 mnist_train_small.csv\n",
            "-rw-r--r-- 1 root root   301141 Jul 16 13:20 california_housing_test.csv\n",
            "-rw-r--r-- 1 root root  1706430 Jul 16 13:20 california_housing_train.csv\n",
            "-rwxr-xr-x 1 root root     1697 Jan  1  2000 anscombe.json\n",
            "-rwxr-xr-x 1 root root      930 Jan  1  2000 README.md\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew_eTGrvXlDw"
      },
      "source": [
        "**Spark Session:** SparkSession은 Spark 2.0부터 엔트리 포인트로 사용된다. 그 이전에는 SparkContext가 사용되었다. SparkSession을 이용해 RDD, 데이터 프레임등을 만든다. SparkSession은 SparkSession.builder를 호출하여 생성하며 다양한 함수들을 통해 세부 설정이 가능하다\n",
        "* pandas와 달리 대용량, 병렬이 가능"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vm6tgcPXdnR"
      },
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "# master : 내가 지금 사용하고 싶은 spark cluster host name\n",
        "# local : 로컬 spark cluster를 사용할 것이다.\n",
        "# [*]  : 모든 cpu 사용 -> cpu의 숫자에 따라 partion이 결정 -> 병렬의 성능 결정\n",
        "spark = SparkSession.builder\\\n",
        "        .master(\"local[*]\")\\\n",
        "        .appName('PySpark_Tutorial')\\\n",
        "        .getOrCreate()"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "id": "LSs_1PYaYWxI",
        "outputId": "bb66e780-b69b-4dca-bf65-2be559d360d3"
      },
      "source": [
        "spark"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://a7a3c2cc94c6:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.0.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>PySpark_Tutorial</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7f713de20710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VNHAAJuCZCqE"
      },
      "source": [
        "**Python 객체를 RDD로 변환해보기**\n",
        "\n",
        "**1> Python 리스트 생성**\n",
        "* 데이터의 크기가 커지면 시간이 너무 느려짐"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HhijTd1UY1i9"
      },
      "source": [
        "name_list_json = [ '{\"name\": \"keeyong\"}', '{\"name\": \"benjamin\"}', '{\"name\": \"claire\"}' ]"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hXqf1lC_Zdxf",
        "outputId": "0b02da76-f7a3-40fa-fcde-99ac215e8e7b"
      },
      "source": [
        "for n in name_list_json:\n",
        "  print(n)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"name\": \"keeyong\"}\n",
            "{\"name\": \"benjamin\"}\n",
            "{\"name\": \"claire\"}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1_A6f6Nbr57",
        "outputId": "1e6fa6cb-f7e4-4504-994b-e7b66a359937"
      },
      "source": [
        "import json\n",
        "\n",
        "for n in name_list_json:\n",
        "  jn = json.loads(n)  # 파이썬 dictionary로 변환\n",
        "  print(jn[\"name\"])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keeyong\n",
            "benjamin\n",
            "claire\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKlanTznb75N"
      },
      "source": [
        "**2> 파이썬 리스트를 RDD로 변환. RDD로 변환되는 순간 Spark 클러스터의 서버들에 데이터가 나눠 저장됨 (파티션). 또한 Lazy Execution이 된다는 점 기억**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvTOpLsrZ_I8"
      },
      "source": [
        "# sc = spark.sparkContext\n",
        "# rdd = sc.parallelize(name_list_json)\n",
        "\n",
        "rdd = spark.sparkContext.parallelize(name_list_json)  # rdd를 가지고 의미있는 코드를 실행하는 순간 만들어진다(lazy exeution)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KIMTLkPdk4ul",
        "outputId": "093f8c20-c757-4ab8-b4b6-4746dc0310c3"
      },
      "source": [
        "rdd"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ParallelCollectionRDD[2] at readRDDFromFile at PythonRDD.scala:262"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KmszVYtzaL3Q",
        "outputId": "c0a8280d-fc4b-4411-8d89-8d554edfa189"
      },
      "source": [
        "rdd.count() "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gMLop2SQbMnZ"
      },
      "source": [
        "# rdd에 새로운 명령어를 실행하여 새로운 rdd 객체를 만든다.\n",
        "# 파이썬의 dictionary 타입으로 넣어진다.\n",
        "\n",
        "parsed_rdd = rdd.map(lambda el:json.loads(el))  "
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RAgNR7EWcl2t",
        "outputId": "ed64cfd2-19f8-4e17-916d-79f5fd08afb8"
      },
      "source": [
        "parsed_rdd"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PythonRDD[5] at RDD at PythonRDD.scala:53"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt5SMf4IcoIZ",
        "outputId": "87e2fe0b-f465-4843-8aac-02ac036d99ab"
      },
      "source": [
        "parsed_rdd.collect()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'name': 'keeyong'}, {'name': 'benjamin'}, {'name': 'claire'}]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T1Kmn_qycqQl"
      },
      "source": [
        "parsed_name_rdd = rdd.map(lambda el:json.loads(el)[\"name\"])"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4ZWxTKgJnDm8",
        "outputId": "78f4d924-90f6-4f87-ea21-6bb934dd65bc"
      },
      "source": [
        "# 로컬로 가져오는 collect()할 때 데이터가 너무 크면 오류 발생\n",
        "# print가 안되고 다른 곳에 저장됨\n",
        "\n",
        "parsed_name_rdd.collect()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['keeyong', 'benjamin', 'claire']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McGKfLtWnsTB"
      },
      "source": [
        "**파이썬 리스트를 데이터프레임으로 변환하기**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_WlhiRsKo7j7"
      },
      "source": [
        "from pyspark.sql.types import StringType\n",
        "\n",
        "df = spark.createDataFrame(name_list_json, StringType())"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S2-pyiKGpLZe",
        "outputId": "c8dcf5f2-80b2-44cb-fa23-d2ea29cf74aa"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Sql4KPhppgr",
        "outputId": "75e46581-56aa-4a1f-aded-2cb2334971f9"
      },
      "source": [
        "df.printSchema()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- value: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2XzTjdEpuZ4",
        "outputId": "799826c7-f289-4ec5-ceac-a03b698d3444"
      },
      "source": [
        "df.select('*').collect()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(value='{\"name\": \"keeyong\"}'),\n",
              " Row(value='{\"name\": \"benjamin\"}'),\n",
              " Row(value='{\"name\": \"claire\"}')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmLv4Guwny00",
        "outputId": "275c6ce0-c03b-4146-dab9-af1f1fa7dd9d"
      },
      "source": [
        "df.select('value').collect()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(value='{\"name\": \"keeyong\"}'),\n",
              " Row(value='{\"name\": \"benjamin\"}'),\n",
              " Row(value='{\"name\": \"claire\"}')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93TLUFxgqLrm"
      },
      "source": [
        "from pyspark.sql import Row\n",
        "\n",
        "row = Row(\"name\") # Or some other column name\n",
        "df_name = parsed_name_rdd.map(row).toDF()"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtFEYRG61-L8",
        "outputId": "389f0dfb-f2a8-4c6b-e2ba-2998d121a389"
      },
      "source": [
        "df_name.printSchema()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- name: string (nullable = true)\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MptOIByKsLeG",
        "outputId": "33369154-2f01-4ed1-bf53-179d5141c300"
      },
      "source": [
        "df_name.select('name').collect()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[Row(name='keeyong'), Row(name='benjamin'), Row(name='claire')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DuFXrjyYrTdR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}